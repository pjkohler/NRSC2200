{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import some packages \n",
    "\n",
    "As in often the case, we start our code by importing some Python modules. \n",
    "\n",
    "In this case, we import the ebbbinghaus module which holds a number of helper function that I prepared. We also import numpy so you can use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-3df10703-604d-4e4d-b7e4-0c48cb7e45da",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6318,
    "execution_start": 1643303697123,
    "source_hash": "b97f439c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install tools for getting data off osf, and for fitting the data\n",
    "!pip install psignifit osfclient\n",
    "import sys\n",
    "import shutil\n",
    "# grab ebbinghaus code from github (make sure you have latest version)\n",
    "shutil.rmtree(\"NRSC2200\", ignore_errors=True)\n",
    "!git clone https://github.com/pjkohler/NRSC2200\n",
    "# add ebbinghaus code to path\n",
    "sys.path.append('NRSC2200/pphys_ebbinghaus')\n",
    "# import python modules to use for analysis\n",
    "import ebbinghaus\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get the data from Open Science Foundation\n",
    "The data files are shared in an public repository at the [Open Science Foundation](https://osf.io/s6mxd/]). \n",
    "\n",
    "The code below calls a function\n",
    "\n",
    "    ebbinghaus.fetch_data()\n",
    "\n",
    "that grabs the data and makes it accessible to the current workbook. \n",
    "\n",
    "If you inspect the function (look under \"fetch_data\" in ebbinghaus.py) you will notice the structure of the code\n",
    "\n",
    "    !osf --project s6mxd clone\n",
    "    \n",
    "- \"!\" indicates that this we are running something from the command line, outside Python.\n",
    "- \"s6mxd\" is the ID of the project on OSF.\n",
    "- \"clone\" simply means we want to make a local copy of the files from OSF\n",
    "\n",
    "The command will place the files in the local folder \n",
    "\n",
    "    s6mxd/osfstorage/data\n",
    "    \n",
    "which is where we grab the data from in the next step. The code also checks if files have already been downloaded, and if they have, it will not waste time redownloading.\n",
    "\n",
    "A limitation that I am still trying to work out is that you cannot specify a subset files that you want to grab, you have to get the whole thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebbinghaus.fetch_data(\"s6mxd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and organize the data from each participant\n",
    "\n",
    "This code uses the function\n",
    "    \n",
    "    ebbinghaus.load_data()\n",
    "\n",
    "to load in the individual data files in csv format that each participant, including you, created when they did the experiment. When running an experiment online, you would normally have some sort of back-end that saves the files automatically somewhere, so you do not have to trust your participant to send you the raw data. \n",
    "\n",
    "Note that the function takes two additional inputs, grab_course and grab_term, which allows you to specify the specific course and term that you want to get data from. Here, we just grab data from our course - that should be plenty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-1b1b72db-540a-4bc0-9f0b-9cadc7d133af",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2210,
    "execution_start": 1643303894254,
    "source_hash": "ac864561",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = [\"s6mxd/osfstorage/data\"]\n",
    "info_df = ebbinghaus.load_data(data_dir, grab_course = \"nrsc2200\", grab_term = \"W2025\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Fit psychometric functions to the data\n",
    "\n",
    "This code uses the function\n",
    "    \n",
    "    ebbinghaus.fit_data()\n",
    "    \n",
    "to fit psychometric functions to the data from each participant. This takes a few minutes. \n",
    "\n",
    "After fitting, the data from all participants are stored in a variable type called a *Pandas dataframe*. This variable type is useful in many ways, including that it makes it easy to save the combined data in a new csv file that can be used statistical analysis or figure making outside of Python. You will not be working with this new csv file in this course, instead you will be grabbing the data directly from the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df, same_params = ebbinghaus.fit_data(info_df, \"same\", False)\n",
    "info_df, small_params = ebbinghaus.fit_data(info_df, \"small\", False)\n",
    "\n",
    "# combine the parameters into a single variable\n",
    "fit_params = [same_params, small_params]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-bd8e3025-eef9-45ac-9a0d-88937cf97e97",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1643303863757,
    "source_hash": "4b09fed"
   },
   "source": [
    "## Assignment 2\n",
    "### Question 1 (4 pts): \n",
    "\n",
    "The code \n",
    "\n",
    "        ebbinghaus.plot_ps(info_df, fit_params, participant_id)\n",
    "    \n",
    "creates two plots, one of the reaction time and one of the responses. The input arguments info_df and fit_params should already be on the workspace if you have run steps 1-5 of the code, and the argument participant_id is a string that indicates the participant ID you want to plot. You can also pass \"means\" to participant_id and the function will plot the average across all participants.\n",
    "\n",
    "Reaction times and responses are plotted seperately for each of the seven physical sizes used for the inner *test disc* (in pixels): 20, 23, 24, 25, 26, 27, 30. The physical size of the inner *reference disc* was always 25 pixels. The same inducers condition is shown in blue, and the small inducers in orange. \n",
    "\n",
    "**(A)** Please use the function <span style=\"color: green; font-weight:bold\">plot_ps</span> to plot the averages across all participants. \n",
    "\n",
    "**(B)** Then use the function <span style=\"color: green; font-weight:bold\">plot_ps</span> to plot your own data - input your own participant ID to the function. If your data was excluded you may plot someone else's ID. Use the command:\n",
    "\n",
    "        print(info_df.ID)\n",
    "\n",
    "to get a list of IDs in the experiment. Try a few, and find someone with reasonable looking data. \n",
    "\n",
    "**(C)** Then draw, by hand, S-shaped *Psychometric functions* through the data, and indicate the approximate *Point of Subjective Equality* (PSE). Do this separately for the same and small inducer conditions, in different colors. Do this for both the average data and your own data.  \n",
    "\n",
    "**(D)** Based on what you did in 1a-1c, is your effect size, measured using the PSEs, bigger than the average or smaller than the average? Explain why. \n",
    "\n",
    "**Please share the code used for A and B (2 lines per question, at most) and screenshots of the *Responses* part of the plot with your hand drawn Psychometric functions, as well as your written response to D, in your submitted assignment.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on your answer here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (2 pts): \n",
    "\n",
    "The code \n",
    "\n",
    "        pse_data = np.array(info_df[[\"pse-same\",\"pse-small\"]])\n",
    "\n",
    "grabs data from the Pandas dataframe to create a 2 x n numpy array that contains the PSEs for the same and small inducers condition, respectively. \n",
    "\n",
    "**(A)** Please write a function <span style=\"color: orange; font-weight:bold\">subject_count</span> that takes **pse_data** as input and applies the *shape* method to get the number of participants in your dataset and return that as an integer variable.\n",
    "    \n",
    "**(B)** Please write a function <span style=\"color: orange; font-weight:bold\">summary_stats</span> that takes **pse_data** as input and uses the methods *mean* and *std* or their corresponding numpy commands to compute the **mean** and **standard deviation**, seperately for the two conditions, and returns them as two separate variables. Note that both mean and standard deviation can be computed in one line of code.\n",
    "\n",
    "**(C)** Please write a function <span style=\"color: orange; font-weight:bold\">compute_err</span> that takes **pse_data** as input and computes the **standard error** and 95% confidence interval, seperately for the two conditions, and returns them as two separate variables. Watch this [video](https://www.youtube.com/watch?v=AQy11Hfp_dU) (also on eClass) to learn how to compute **standard error** using the sample size (=number of subjects), and how to convert standard error to the 95% confidence interval. You can use the command np.sqrt to take the square root of a number. Your function should return two variables named:\n",
    "    \n",
    "        pse_stderr # standard error\n",
    "        pse_ci # 95% confidence interval\n",
    "\n",
    "**(D)** Then use the following code to plot your data as a bar plot with error bars. Try replacing pse_ci with pse_stderr in the below and observe how the error bars change:\n",
    "    \n",
    "        plt.bar((0,1), pse_means, yerr=pse_stderr, capsize=5)\n",
    "        plt.ylim([15,30])\n",
    "\n",
    "**Please submit the functions created for A, B and C for checking using VPL. Please share a screenshot of your bar plot created in D in your submitted assignment.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on your answer here:\n",
    "\n",
    "# (A)\n",
    "def subject_count(pse_data):\n",
    "    # your code here\n",
    "    return(num_subs)\n",
    "\n",
    "num_subs = subject_count(pse_data)\n",
    "\n",
    "# (B)\n",
    "def summary_stats(pse_data):\n",
    "    # your code here\n",
    "    return(pse_mean, pse_stdev)\n",
    "\n",
    "pse_mean, pse_stdev = summary_stats(pse_data)\n",
    "\n",
    "# (C)\n",
    "def compute_err(pse_data):\n",
    "    # your code here\n",
    "    return(pse_stderr, pse_ci)\n",
    "\n",
    "pse_stderr, pse_ci = compute_err(pse_data)\n",
    "\n",
    "# (D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (2 pts): \n",
    "\n",
    "The size of the *Ebbinghaus effect* for each participant can be computed as the difference between pse_same and pse_small. \n",
    "\n",
    "**(A)** Write a function <span style=\"color: orange; font-weight:bold\">effect_sizes</span> that takes pse_data as input and subtracts pse_small from pse_same and assign the output to a new variable. This can be done in one line by selecting the different columns of pse_data. Now use the methods *max* and *min* or the corresponding numpy commands to compute the maximum and minimum effect sizes. Have the function returns them as two distinct variables. \n",
    "\n",
    "**(B)** Write a similar function <span style=\"color: orange; font-weight:bold\">effect_ids</span> that again takes pse_data as input, and subtracts pse_small from pse_same to create a new variable, but instead uses the methods *argmax* and *argmin* or the corresponding numpy commands to get the index of the participant that has the maximum and minimum effect size. Have the function returns them as two distinct variables. \n",
    "\n",
    "You can then get the ID of the participants with the minimum or maximum effect sizes using this command\n",
    "    \n",
    "    print(np.array(info_df[[\"ID\"]])[max_id,0])\n",
    "\n",
    "where max_id is the index. Same approach for min_id.\n",
    "\n",
    "**Please submit the functions created for A and B, for checking using VPL. Please report the ID of the participant with the maximum and minimum effects in your submitted assignment.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on your answer here:\n",
    "\n",
    "# (A)\n",
    "def effect_sizes(pse_data):\n",
    "    # your code here\n",
    "    return(pse_max, pse_min)\n",
    "\n",
    "pse_max, pse_min = effect_sizes(pse_data)\n",
    "\n",
    "# (B)\n",
    "def effect_ids(pse_data):\n",
    "    # your code here\n",
    "    return(max_id, min_id)\n",
    "\n",
    "max_id, min_id = effect_ids(pse_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (2 pts):\n",
    "\n",
    "Please read the article “The surface area of human V1 predicts the subjective experience of object size”, linked on eClass. \n",
    "\n",
    "Based on the findings presented in the article, what would you expect to be true about primary visual cortex (area V1) of those participants who have the largest Ebbinghaus effects in your experiment?\n",
    "\n",
    "**Please share your answer to this question in your submitted assignment.**"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "cb230051-44ac-4368-ae81-c257a4783978",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
