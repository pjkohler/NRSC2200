{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import some packages \n",
    "\n",
    "As in often the case, we start our code by importing some Python modules. \n",
    "\n",
    "In this case, we import the ebbbinghaus module which holds a number of helper function that I prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-3df10703-604d-4e4d-b7e4-0c48cb7e45da",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6318,
    "execution_start": 1643303697123,
    "source_hash": "b97f439c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import python modules to use for analysis\n",
    "import ebbinghaus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get the data from Open Science Foundation\n",
    "The data files are shared in an public repository at the [Open Science Foundation](https://osf.io/s6mxd/]). \n",
    "\n",
    "The code below calls a function\n",
    "\n",
    "    ebbinghaus.fetch_data()\n",
    "\n",
    "that grabs the data and makes it accessible to the current workbook. \n",
    "\n",
    "If you inspect the function (look under \"fetch_data\" in ebbinghaus.py) you will notice the structure of the code\n",
    "\n",
    "    !osf --project s6mxd clone\n",
    "    \n",
    "- \"!\" indicates that this we are running something from the command line, outside Python.\n",
    "- \"s6mxd\" is the ID of the project on OSF.\n",
    "- \"clone\" simply means we want to make a local copy of the files from OSF\n",
    "\n",
    "The command will place the files in the local folder \n",
    "\n",
    "    s6mxd/osfstorage/data\n",
    "    \n",
    "which is where we grab the data from in the next step. The code also checks if files have already been downloaded, and if they have, it will not waste time redownloading.\n",
    "\n",
    "A limitation that I am still trying to work out is that you cannot specify a subset files that you want to grab, you have to get the whole thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebbinghaus.fetch_data(\"s6mxd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and organize the data from each participant\n",
    "\n",
    "This code uses the function\n",
    "    \n",
    "    ebbinghaus.load_data()\n",
    "\n",
    "to load in the individual data files in csv format that each participant, including you, created when they did the experiment. When running an experiment online, you would normally have some sort of back-end that saves the files automatically somewhere, so you do not have to trust your participant to send you the raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-1b1b72db-540a-4bc0-9f0b-9cadc7d133af",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2210,
    "execution_start": 1643303894254,
    "source_hash": "ac864561",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = [\"s6mxd/osfstorage/data\"]\n",
    "info_df = ebbinghaus.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Fit psychometric functions to the data\n",
    "\n",
    "This code uses the function\n",
    "    \n",
    "    ebbinghaus.fit_data()\n",
    "    \n",
    "to fit psychometric functions to the data from each participant. This takes a few minutes. \n",
    "\n",
    "After fitting, the data from all participants are stored in a variable type called a *Pandas dataframe*. This variable type is useful in many ways, including that it makes it easy to save the combined data in a new csv file that can be used statistical analysis or figure making outside of Python. You will not be working with this new csv file in this course, instead you will be grabbing the data directly from the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df, same_params, same_options = ebbinghaus.fit_data(info_df, \"same\", False)\n",
    "info_df, small_params, small_options = ebbinghaus.fit_data(info_df, \"small\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-bd8e3025-eef9-45ac-9a0d-88937cf97e97",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1643303863757,
    "source_hash": "4b09fed"
   },
   "source": [
    "<style type=\"text/css\">\n",
    "    ol { list-style-type: upper-alpha}\n",
    "    o {color: orange; font-weight: bold}\n",
    "    g {color: green; font-weight: bold}\n",
    "</style>\n",
    "\n",
    "## Assignment 2\n",
    "### Question 1 (4 pts): \n",
    "\n",
    "The code \n",
    "\n",
    "    ebbinghaus.plot_ps(info_df, fit_params, participant_id)\n",
    "    \n",
    "creates two plots, one of the reaction time and one of the responses. The input arguments info_df and fit_params should already be on the workspace if you have run steps 1-5 of the code, and the argument participant_id is a string that indicates the participant ID you want to plot. You can also pass \"means\" to participant_id and the function will plot the average across all participants.\n",
    "\n",
    "Reaction times and responses are plotted seperately for each of the seven physical sizes used for the inner *test disc* (in pixels): 20, 23, 24, 25, 26, 27, 30. The physical size of the inner *reference disc* was always 25 pixels. The same inducers condition is shown in blue, and the small inducers in orange. \n",
    "\n",
    "**(A)** Please use the function <g>plot_ps</g> to plot the averages across all participants. \n",
    "\n",
    "**(B)** Then use the function <g>plot_ps</g> to plot your own data - input your own participant ID to the function. If your data was excluded you may plot someone else's ID. Participant \"999\" has very reasonable data. \n",
    "\n",
    "**(C)** Then draw, by hand, S-shaped *Psychometric functions* through the data, and indicate the approximate *Point of Subjective Equality* (PSE). Do this separately for the same and small inducer conditions, in different colors. Do this for both the average data and your own data.  \n",
    "\n",
    "**(D)** Based on what you did in 1a-1c, is your effect size, measured using the PSEs, bigger than the average or smaller than the average? Explain why. \n",
    "\n",
    "Your answer should include the code used in a and b (2 lines per question, at most) and screenshots of the *Responses* part of the plot with your hand drawn functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style type=\"text/css\">\n",
    "    ol { list-style-type: upper-alpha}\n",
    "    o {color: orange; font-weight: bold}\n",
    "    g {color: green; font-weight: bold}\n",
    "</style>\n",
    "### Question 2 (2 pts): \n",
    "\n",
    "The code \n",
    "\n",
    "        pse_data = np.array(info_df[[\"pse-same\",\"pse-small\"]])\n",
    "\n",
    "grabs data from the Pandas dataframe to create a 2 x n numpy array that contains the PSEs for the same and small inducers condition, respectively. \n",
    "\n",
    "**(A)** Please write a function <g>subject_count</g> that takes **pse_data** as input and applies the *shape* method to get the number of participants in your dataset and return that as an integer variable.\n",
    "    \n",
    "**(B)** Please write a function <g>summary_stats</g> that takes **pse_data** as input and uses the methods *mean* and *std* or their corresponding numpy commands to compute the **mean** and **standard deviation**, seperately for the two conditions, and returns them as two separate variables. Note that both mean and standard deviation can be computed in one line of code.\n",
    "\n",
    "**(C)** Please write a function <g>compute_err</g> that takes **pse_data** as input and computes the **standard error** and 95% confidence interval, seperately for the two conditions, and returns them as two separate variables. Watch this [video](https://www.youtube.com/watch?v=AQy11Hfp_dU) (also on eClass) to learn how to compute **standard error** using the sample size (=number of subjects), and how to convert standard error to the 95% confidence interval. You can use the command *np.sqrt* to take the square root of a number. Your function should return two variables named:\n",
    "    \n",
    "        pse_stderr # standard error\n",
    "        pse_ci # 95% confidence interval\n",
    "\n",
    "**(D)** Then use the following code to plot your data as a bar plot with error bars. Try replacing pse_ci with pse_stderr in the above and observe how the error bars change:\n",
    "    \n",
    "        plt.bar((0,1), pse_means, yerr=pse_ci, capsize=5)\n",
    "        plt.ylim([15,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on your answer here:\n",
    "\n",
    "# (A)\n",
    "def subject_count(pse_data):\n",
    "    # your code here\n",
    "    return(num_subs)\n",
    "\n",
    "num_subs = subject_count(pse_data)\n",
    "\n",
    "# (B)\n",
    "def summary_stats(pse_data):\n",
    "    # your code here\n",
    "    return(pse_mean, pse_stdev)\n",
    "\n",
    "pse_mean, pse_stdev = summary_stats(pse_data)\n",
    "\n",
    "# (C)\n",
    "def compute_err(pse_data):\n",
    "    # your code here\n",
    "    return(pse_stderr, pse_ci)\n",
    "\n",
    "pse_stderr, pse_ci = compute_err(pse_data)\n",
    "\n",
    "# (D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style type=\"text/css\">\n",
    "    ol { list-style-type: upper-alpha}\n",
    "    o {color: orange; font-weight: bold}\n",
    "    g {color: green; font-weight: bold}\n",
    "</style>\n",
    "### Question 3 (2 pts): \n",
    "\n",
    "The size of the *Ebbinghaus effect* for each participant can be computed as the difference between pse_same and pse_small. \n",
    "\n",
    "**(A)** Write a function <g>effect_sizes</g> that takes pse_data as input and subtracts pse_small from pse_same and assign the output to a new variable. This can be done in one line by selecting the different columns of pse_data. Now use the methods *max* and *min* or the corresponding numpy commands to compute the maximum and minimum effect sizes. Have the function returns them as two distinct variables. \n",
    "\n",
    "**(B)** Write a similar function <g>effect_ids</g> that again takes pse_data as input, and subtracts pse_small from pse_same to create a new variable, but instead uses the methods *max* and *min* or the corresponding numpy commands to get the index of the participant that has the maximum and minimum effect size. Have the function returns them as two distinct variables. \n",
    "\n",
    "You can then get the ID of the participants with the minimum or maximum effect sizes using this command\n",
    "    \n",
    "    print(np.array(info_df[[\"ID\"]])[max_id,0])\n",
    "\n",
    "where max_id is the index. Same approach for min_id.\n",
    "\n",
    "**Your answer should include the code used to create the two functions. The functions can be tested using VPL, on eClass.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on your answer here:\n",
    "\n",
    "# (A)\n",
    "def effect_sizes(pse_data):\n",
    "    # your code here\n",
    "    return(pse_max, pse_min)\n",
    "\n",
    "pse_max, pse_min = effect_sizes(pse_data)\n",
    "\n",
    "# (B)\n",
    "def effect_ids(pse_data):\n",
    "    # your code here\n",
    "    return(max_id, min_id)\n",
    "\n",
    "max_id, min_id = effect_sizes(pse_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (2 pts):\n",
    "\n",
    "Please read the article “The surface area of human V1 predicts the subjective experience of object size”, linked on eClass. \n",
    "\n",
    "Based on the findings presented in the article, what would you expect to be true about primary visual cortex (area V1) of those participants who have the largest Ebbinghaus effects in your experiment?"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "cb230051-44ac-4368-ae81-c257a4783978",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
