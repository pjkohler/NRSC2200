{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import some packages \n",
    "\n",
    "As in often the case, we start our code by importing some Python modules. \n",
    "\n",
    "In this case, we import the objects module which holds a number of helper function that I prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-3df10703-604d-4e4d-b7e4-0c48cb7e45da",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6318,
    "execution_start": 1643303697123,
    "source_hash": "b97f439c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 'objects' module has code that I wrote to analyze this dataset\n",
    "import objects\n",
    "# matplotlib.pyplot module has functions for plotting, note that we are importing it as plt\n",
    "import matplotlib.pyplot as plt\n",
    "# import numpy for working with matrices\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get the data from Open Science Foundation\n",
    "The data files are shared in an public repository at the [Open Science Foundation](https://osf.io/bwntz/]). \n",
    "\n",
    "The code below calls a function\n",
    "\n",
    "    objects.fetch_data()\n",
    "\n",
    "that grabs the data and makes it accessible to the current workbook. \n",
    "\n",
    "If you inspect the function (look under \"fetch_data\" in objects.py) you will notice the structure of the code\n",
    "\n",
    "    !osf --project bwntz clone\n",
    "    \n",
    "- \"!\" indicates that this we are running something from the command line, outside Python.\n",
    "- \"bwntz\" is the ID of the project on OSF.\n",
    "- \"clone\" simply means we want to make a local copy of the files from OSF\n",
    "\n",
    "The command will place the files in the local folder \n",
    "\n",
    "    bwntz/osfstorage/data\n",
    "    \n",
    "which is where we grab the data from in the next step. The code also checks if files have already been downloaded, and if they have, it will not waste time redownloading.\n",
    "\n",
    "A limitation that I am still trying to work out is that you cannot specify a subset files that you want to grab, you have to get the whole thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"bwntz\"\n",
    "objects.fetch_data(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and organize the data from each participant\n",
    "\n",
    "The code below loads in the individual data files in csv format that each participant, including you, created when they did the experiment. When running an experiment online, you would normally have some sort of back-end that saves the files automatically somewhere, so you do not have to trust your participant to send you the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-1b1b72db-540a-4bc0-9f0b-9cadc7d133af",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2210,
    "execution_start": 1643303894254,
    "source_hash": "ac864561",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"bwntz/osfstorage/data\"\n",
    "labels = ['bear','elephant','person','car','dog','apple','chair','plane','bird','zebra']\n",
    "all_data, sub_list, conf_labels = objects.load_data(data_dir, grab_course = \"nrsc2200\", grab_term = \"W2025\", labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Average confusion matrices and accuracies over participants\n",
    "You may re-run this code after removing subjects from all_conf and all_corr that more than two standard errors away from the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_conf, mean_corr, all_conf, all_corr = objects.confusion_averages(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Plot accuracies for each image pair\n",
    "Now we will plot the accuracies for each image pair. We will use the values from the confusion matrix, but convert them into a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_list, label_list = objects.confusion_list(mean_conf, conf_labels)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5))\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 12}\n",
    "plt.rc('font', **font)\n",
    "plt.plot(conf_list)\n",
    "ax.set_xticks(range(0,len(label_list)+1,10))\n",
    "#ax.set_xticklabels(label_list[0:10:],rotation = 45, ha=\"center\")\n",
    "plt.xlabel(\"image pair\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create a confusion matrix\n",
    "The code below makes a confusion matrix. Each cell reflects the proportion of trials where the non-target image (distractor) was chosen for each image pair. Since the target and distractor were always different, the diagonal is colored white and marked with \"NA\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-e5a56cd0-53af-4308-be9f-7429449b1c29",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 47,
    "execution_start": 1643303734785,
    "source_hash": "946e5b8"
   },
   "outputs": [],
   "source": [
    "# reorder the labels to reveal category-specific confusion\n",
    "new_labels = ['bear','bird','dog','elephant','zebra','person','apple','car','chair','plane']\n",
    "plt = objects.confusion_plot(mean_conf, labels, new_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "\n",
    "### Question 1 (4 pts)\n",
    "\n",
    "In the previous assignment you identified outlier timepoints. In some cases, it may be similarly useful to identify outlier participants. This is especially true for behavioral experiments that are collected online, where you may have participants that do not understand the task, or simply decide not to do it. Here we will identify participants whose overall accuracy is substantially lower than the mean. \n",
    "\n",
    "**(A)** Write a function called <span style=\"color: orange; font-weight:bold\">quality_control</span> that takes *all_corr* as input and returns *reject_idx*, an one-dimensional numpy array that has *True* for any participant whose accuracy is more than 2 standard errors **lower** than the mean, and *False* otherwise. *reject_idx* should have n elements, where n is the number of participants. \n",
    "\n",
    "**(B)** Pass that variable as the second input to the function <span style=\"color: green; font-weight:bold\">objects.confusion_averages</span> to recompute the variables *mean_conf*, *mean_corr*, *all_conf* and *all_corr* as in Step 4 above.\n",
    "\n",
    "**(C)** Use the recomputed average to plot the confusion matrix - this can be done by re-running <span style=\"color: green; font-weight:bold\">objects.confusion_plot</span> from **Step 6** with the recomputed *mean_conf* - the other input variables can stay the same. Take a screenshot of your confusion matrix.\n",
    "\n",
    "**(D)** Write a function called <span style=\"color: orange; font-weight:bold\">rejected_ids</span> that takes *sub_list* and *reject_idx* as input and returns a variable *outliers*, which should be a one-dimensional numpy array with the IDs of the subjects that were rejected. \n",
    "\n",
    "**In your submitted assignment, please share the following and only the following: The code used in B and C, including your call to the quality_control function. The screenshot of the confusion matrix generated in C. Your call to the rejected_ids function and the IDs of the rejected participants. Please submit the functions created for A and D, for checking using VPL.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on your answer here\n",
    "\n",
    "# (A)\n",
    "def quality_control(all_corr):\n",
    "    # your code here\n",
    "    return(reject_idx)\n",
    "\n",
    "# (B)\n",
    "\n",
    "# (C)\n",
    "\n",
    "# (D)\n",
    "def rejected_ids(sub_list, reject_list):\n",
    "    # your code here\n",
    "    return(outliers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (2 pts)\n",
    "\n",
    "One of key points of the experiment and the paper *Simple Learned Weighted Sums of Inferior Temporal Neuronal Firing Rates Accurately Predict Human Core Object Recognition Performance* by Majaj and colleagues (2015), is that some objects are more difficult to distinguish than others. Here we will identify those objects.\n",
    "\n",
    "**(A)** Pay attention **Step 5: Plot accuracies for each image pair**. Note that the variable *conf_list* is a sorted list of accuracies for each image pair. Also note that *label_list* is a matched list of image pair labels in the format \"tt_vs_dd\" where tt is the target and dd is the distractor. Write a function <span style=\"color: orange; font-weight:bold\">image_extremes</span> that takes *conf_list* and *label_list* as input and returns the image pairs with the lowest and highest average accuracy, i.e. the distractors that were easiest and most difficult to distinguish from the targets. The two outputs should be string variables. \n",
    "\n",
    "**(B)** Find the image pairs you identified in the confusion matrix plotted in **Step 6**. Are your findings consistent with the confusion matrix? Explain why.\n",
    "\n",
    "**In your submitted assignment, please share the following and only the following: The code used in A, including your call to the image_extremes function and a print statement printing the two outputs, as well as the result of that print statement. Your answer to B, in 3-5 sentences or so. Please submit your function created for B, for checking using VPL.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on your answer here:\n",
    "def image_extremes(conf_list, label_list):\n",
    "    # your code here\n",
    "    return(min_pair, max_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (4 pts)\n",
    "\n",
    "It is often informative to establish the consistency of your measurement across participants. Here we will do something like this, by computing the split-half correlation across participants. In this assignment you will create a 3 functions that can be tested on eClass, and there will be a short answer question as well. \n",
    "\n",
    "**(A)** The variable *all_conf_new* holds the confusion matrices for each participant. It is a n x 10 x 10 matrix, where n is the remaining number of subjects after removing outlier participants in **Question 1**. Write a function <span style=\"color: orange; font-weight:bold\">split_half</span> that takes a three-dimensional numpy array like that as input and returns *c1* and *c2*, two average confusion matrices, one computed over the first half of participants and one over the second half. Because the confusion matrices contain missing values, NaNs, you have to use the command **np.nanmean** to compute the averages. Both outputs (*c1* and *c2*) should be 10 x 10 numpy arrays.\n",
    "\n",
    "**(B)** Write a function, <span style=\"color: orange; font-weight:bold\">compute_corr</span>, that takes two average confusion matrices (e.g., *c1* and *c2*) as input and uses the function **np.corrcoef** to compute the Pearson correlation between those two confusion matrices. The function should return the output of np.corrcoef as a variable *p_c*. *p_c* should be a 2 x 2 numpy arrays (see next question). Note that your average confusion matrices will contain NaNs, so you have to remove them before applying np.corrcoef. Assuming your average is stored in variable c1, this can be done with the following code:\n",
    "\n",
    "        c1[~np.isnan(c1)]\n",
    "   \n",
    "**(C)** **np.corrcoef** returns a 2 x 2 numpy array, with two values being equal to 1. The other two values are identical and both equal the Pearson correlation. Write a function <span style=\"color: orange; font-weight:bold\">grab_corr</span> that takes the *p_c* as input and returns a single float number with the correlation. Use your functions to print the correlation. \n",
    "\n",
    "**(D)** In the Majaj paper the median correlation across human participants in a similar task was 0.929. Now, it is certainly possible that we simply took the task less seriously than Majaj's participants, but if we give ourselves some credit and assume that this is not the case, what are some other plausible reasons our split-half correlation was worse? \n",
    "\n",
    "**In your submitted assignment, please share the following and only the following: The code used in C, including your calls to split_half, compute_corr and grab_corr, as well as the print statement for printing the correlation, and the result of that statement. Your answer to D, in 5 sentences or so. Please submit your functions created for A, B and C, for checking using VPL.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on your answer here:\n",
    "# (A)\n",
    "def split_half(all_conf):\n",
    "    # your code here\n",
    "    return(c1, c2)\n",
    "\n",
    "# (B)\n",
    "def compute_corr(c1, c2):\n",
    "    # your code here\n",
    "    return(p_c)\n",
    "\n",
    "# (C)\n",
    "def grab_corr(p_c):\n",
    "    # your code here\n",
    "    return(out)"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "cb230051-44ac-4368-ae81-c257a4783978",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
