{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import some packages \n",
    "\n",
    "As in often the case, we start our code by importing some Python modules. \n",
    "\n",
    "In this case, we import the objects module which holds a number of helper function that I prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00000-3df10703-604d-4e4d-b7e4-0c48cb7e45da",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6318,
    "execution_start": 1643303697123,
    "source_hash": "b97f439c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 'objects' module has code that I wrote to analyze this dataset\n",
    "import objects\n",
    "# matplotlib.pyplot module has functions for plotting, note that we are importing it as plt\n",
    "import matplotlib.pyplot as plt\n",
    "# import numpy for working with matrices\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get the data from Open Science Foundation\n",
    "The data files are shared in an public repository at the [Open Science Foundation](https://osf.io/bwntz/]). \n",
    "\n",
    "The code below calls a function\n",
    "\n",
    "    objects.fetch_data()\n",
    "\n",
    "that grabs the data and makes it accessible to the current workbook. \n",
    "\n",
    "If you inspect the function (look under \"fetch_data\" in objects.py) you will notice the structure of the code\n",
    "\n",
    "    !osf --project bwntz clone\n",
    "    \n",
    "- \"!\" indicates that this we are running something from the command line, outside Python.\n",
    "- \"bwntz\" is the ID of the project on OSF.\n",
    "- \"clone\" simply means we want to make a local copy of the files from OSF\n",
    "\n",
    "The command will place the files in the local folder \n",
    "\n",
    "    bwntz/osfstorage/data\n",
    "    \n",
    "which is where we grab the data from in the next step. The code also checks if files have already been downloaded, and if they have, it will not waste time redownloading.\n",
    "\n",
    "A limitation that I am still trying to work out is that you cannot specify a subset files that you want to grab, you have to get the whole thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"bwntz\"\n",
    "objects.fetch_data(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and organize the data from each participant\n",
    "\n",
    "The code below loads in the individual data files in csv format that each participant, including you, created when they did the experiment. When running an experiment online, you would normally have some sort of back-end that saves the files automatically somewhere, so you do not have to trust your participant to send you the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00001-1b1b72db-540a-4bc0-9f0b-9cadc7d133af",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2210,
    "execution_start": 1643303894254,
    "source_hash": "ac864561",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"bwntz/osfstorage/data\"\n",
    "exp_n = 199\n",
    "labels = ['bear','elephant','person','car','dog','apple','chair','plane','bird','zebra']\n",
    "all_data, sub_list, conf_labels = objects.load_data(data_dir, exp_n, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Average confusion matrices and accuracies over participants\n",
    "You may re-run this code after removing subjects from all_conf and all_corr that more than two standard errors away from the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_conf, mean_corr, all_conf, all_corr = objects.confusion_averages(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Plot accuracies for each image pair\n",
    "Now we will plot the accuracies for each image pair. We will use the values from the confusion matrix, but convert them into a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_list, label_list = objects.confusion_list(mean_conf, conf_labels)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5))\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 12}\n",
    "plt.rc('font', **font)\n",
    "plt.plot(conf_list)\n",
    "ax.set_xticks(range(0,len(label_list)+1,10))\n",
    "#ax.set_xticklabels(label_list[0:10:],rotation = 45, ha=\"center\")\n",
    "plt.xlabel(\"image pair\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create a confusion matrix\n",
    "The code below makes a confusion matrix. Each cell reflects the proportion of trials where the non-target image (distractor) was chosen for each image pair. Since the target and distractor were always different, the diagonal is colored white and marked with \"NA\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-e5a56cd0-53af-4308-be9f-7429449b1c29",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 47,
    "execution_start": 1643303734785,
    "source_hash": "946e5b8"
   },
   "outputs": [],
   "source": [
    "# reorder the labels to reveal category-specific confusion\n",
    "new_labels = ['bear','bird','dog','elephant','zebra','person','apple','car','chair','plane']\n",
    "plt = objects.confusion_plot(mean_conf, labels, new_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "    ol { list-style-type: upper-alpha}\n",
    "    o {color: orange; font-weight: bold}\n",
    "    g {color: green; font-weight: bold}\n",
    "</style>\n",
    "\n",
    "## Assignment 4\n",
    "\n",
    "### Question 1 (4 pts)\n",
    "\n",
    "In the previous assignment you identified outlier timepoints. In some cases, it may be similarly useful to identify outlier participants. This is especially true for behavioral experiments that are collected online, where you may have participants that do not understand the task, or simply decide not to do it. Here we will identify participants whose overall accuracy is substantially lower than the mean. \n",
    "\n",
    "1. Write a function called <o>quality_control</o> that takes *all_corr* as input creates a variable *reject_list* that has *True* for any participant whose accuracy is more than 2 standard errors **lower** than the mean, and *False* otherwise. \n",
    "\n",
    "2. Pass that variable as the second input to the function <g>objects.confusion_averages</g> to recompute the variables *mean_conf*, *mean_corr*, *all_conf* and *all_corr* as in Step 4 above.\n",
    "\n",
    "3. Use the recomputed average to plot the confusion matrix - this can be done by re-running <g>objects.confusion_plot</g> from **Step 6** with the recomputed *mean_conf* - the other input variables can stay the same. Take a screenshot of your confusion matrix and share.\n",
    "\n",
    "4. Write a function called <o>rejected_ids</o> that takes *sub_list* and *reject_list* as input and returns the participants that were rejected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on your answer here\n",
    "\n",
    "# (a)\n",
    "def quality_control(all_corr):\n",
    "    # your code here\n",
    "    return(reject_list)\n",
    "\n",
    "reject_list = quality_control(all_corr)\n",
    "\n",
    "# (b)\n",
    "\n",
    "# (c)\n",
    "\n",
    "# (d)\n",
    "def rejected_ids(sub_list, reject_list):\n",
    "    # your code here\n",
    "    return(outliers)\n",
    "\n",
    "outliers = rejected_ids(sub_list, reject_list)\n",
    "print(\"rejected participants: \", outliers) \n",
    "print(\"old percent correct: {:.2}, new percent correct: {:.2}\".format(mean_corr, mean_corr_new))\n",
    "print(\"total n: {}, n after rejection: {}\".format(all_conf.shape[0], all_conf_new.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "    ol { list-style-type: upper-alpha}\n",
    "    o {color: orange; font-weight: bold}\n",
    "    g {color: green; font-weight: bold}\n",
    "</style>\n",
    "\n",
    "### Question 2 (2 pts)\n",
    "\n",
    "One of key points of the experiment and the paper *Simple Learned Weighted Sums of Inferior Temporal Neuronal Firing Rates Accurately Predict Human Core Object Recognition Performance* by Majaj and colleagues (2015), is that some objects are more difficult to distinguish than others. Here we will identify those objects.\n",
    "\n",
    "1. Pay attention **Step 5: Plot accuracies for each image pair**. Note that the variable *conf_list* is a sorted list of accuracies for each image pair. Also note that *label_list* is a matched list of image pair labels in the format \"tt_vs_dd\" where tt is the target and dd is the distractor. Write a function <o>image_extremes</o> that takes *conf_list* and *label_list* as input and returns the image pairs with the lowest and highest average accuracy, i.e. the distractors that were easiest and most difficult to distinguish from the targets. Share your code, and the resulting image pairs.\n",
    "\n",
    "2. Find the image pairs you identified in the confusion matrix plotted in **Step 6**. Are your findings consistent with the confusion matrix? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on your answer here:\n",
    "def image_extremes(conf_list, label_list):\n",
    "    # your code here\n",
    "    return(min_pair, max_pair)\n",
    "\n",
    "min_pair, max_pair = image_extremes(conf_list, label_list)\n",
    "print(min_pair, max_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "    ol { list-style-type: upper-alpha}\n",
    "    o {color: orange; font-weight: bold}\n",
    "    g {color: green; font-weight: bold}\n",
    "</style>\n",
    "\n",
    "### Question 3 (4 pts)\n",
    "\n",
    "It is often informative to establish the consistency of your measurement across participants. Here we will do something like this, by computing the split-half correlation across participants. In this assignment you will create a 3 functions that can be tested on eClass, and there will be a short answer question as well. \n",
    "\n",
    "1. The variable *all_conf_new* holds the confusion matrices for each participant. It is a n x 10 x 10 matrix, where n is the remaining number of subjects after removing outlier participants in **Question 1**. Write a function <o>split_half</o> that takes a three-dimensional array variable like that as input and returns *c1* and *c2*, two average confusion matrices, one computed over the first half of participants and one over the second half. Because the confusion matrices contain missing values, NaNs, you have to use the command np.nanmean to compute the averages.\n",
    "\n",
    "2. Write a function, <o>compute_corr</o>, that takes two average confusion matrices (e.g., *c1* and *c2*) as input and uses the function np.corrcoef to compute the Pearson correlation between those two confusion matrices. The function should return the output of np.corrcoef as a variable *p_c*. Note that your average confusion matrices will contain NaNs, so you have to remove them before applying np.corrcoef. Assuming your average is stored in variable c1, this can be done with the following code:\n",
    "\n",
    "        c1[~np.isnan(c1)]\n",
    "   \n",
    "3. np.corrcoef returns a 2 x 2 matrix, with two values being equal to 1. The other two values are identical and both equal the Pearson correlation. Write a function <o>grab_corr</o> that takes the *p_c* as input and returns a single float number with the correlation.\n",
    "\n",
    "4. Use your functions to print the correlation. In the Majaj paper the median correlation across human participants in a similar task was 0.929. Why do you think we did worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on your answer here:\n",
    "# (A)\n",
    "def split_half(all_conf):\n",
    "    # your code here\n",
    "    return(c1, c2)\n",
    "\n",
    "c1, c2 = split_half(all_conf_new)\n",
    "\n",
    "# (B)\n",
    "def compute_corr(c1, c2):\n",
    "    # your code here\n",
    "    return(p_c)\n",
    "p_c = compute_corr(c1, c2)\n",
    "\n",
    "# (C)\n",
    "def grab_corr(p_c):\n",
    "    # your code here\n",
    "    return(out)\n",
    "\n",
    "p_c = grab_corr(p_c)\n",
    "\n",
    "print(p_c)"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "cb230051-44ac-4368-ae81-c257a4783978",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
