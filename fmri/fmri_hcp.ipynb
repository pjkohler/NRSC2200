{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxn5tK5icpbe"
   },
   "source": [
    "# Human Connectome Project: Task data + Behavioral data\n",
    "The Human Connectome Project dataset comprises task-based fMRI from a large sample of human subjects. The NMA-curated version of this dataset includes time series data that has been preprocessed and spatially-downsampled by aggregating within 360 regions of interest. \n",
    "\n",
    "In order to use this dataset, please electronically sign the HCP data use terms at [ConnectomeDB](https://db.humanconnectome.org). Instructions for this are on pp. 24-25 of the [HCP Reference Manual](https://www.humanconnectome.org/storage/app/media/documentation/s1200/HCP_S1200_Release_Reference_Manual.pdf).\n",
    "\n",
    "This Notebook provides code for downloading the data and doing some basic visualisation and processing.\n",
    "\n",
    "The Notebook was created by Neuromatch Academy (original version available [here](https://github.com/NeuromatchAcademy/course-content)) and edited by Prof. Peter J. Kohler for use in NRSC-2200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Some Packages\n",
    "As in often the case, we start our code by importing some Python modules. \n",
    "\n",
    "Remember: **Your code will not work** unless you run the cell in which the modules are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXasV6tWdJls"
   },
   "outputs": [],
   "source": [
    "# import Python modules to use for analysis\n",
    "import os, requests, tarfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import the nilearn packages, which will be used for making plots of the whole brain surface\n",
    "from nilearn import plotting, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define some functions\n",
    "We provide three helper functions: \n",
    "\n",
    "* **load_single_timeseries**: Loads the time series from a single suject and a single run\n",
    "\n",
    "* **load_evs**: Loads an EV file for each task. An EV file (EV:Explanatory Variable) describes the task experiment in terms of stimulus onset, duration, and amplitude. These can be used to model the task time series data.\n",
    "\n",
    "* **average_frames**: Averages all frames (TRs) from any given condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_timeseries(subject, experiment, run, remove_mean=True):\n",
    "    \"\"\"Load timeseries data for a single subject and single run.\n",
    "\n",
    "    Args:\n",
    "    subject (str):      subject ID to load\n",
    "    experiment (str):   Name of experiment \n",
    "    run (int):          (0 or 1)\n",
    "    remove_mean (bool): If True, subtract the parcel-wise mean (typically the mean BOLD signal is not of interest)\n",
    "\n",
    "    Returns\n",
    "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
    "\n",
    "    \"\"\"\n",
    "    bold_run  = RUNS[run]\n",
    "    bold_path = f\"{HCP_DIR}/subjects/{subject}/{experiment}/tfMRI_{experiment}_{bold_run}\"\n",
    "    bold_file = \"data.npy\"\n",
    "    ts = np.load(f\"{bold_path}/{bold_file}\")\n",
    "    if remove_mean:\n",
    "        ts -= ts.mean(axis=1, keepdims=True)\n",
    "    return ts\n",
    "\n",
    "\n",
    "def load_evs(subject, experiment, run):\n",
    "    \"\"\"Load EVs (explanatory variables) data for one task experiment.\n",
    "\n",
    "    Args:\n",
    "    subject (str): subject ID to load\n",
    "    experiment (str) : Name of experiment\n",
    "    run (int): 0 or 1\n",
    "\n",
    "    Returns\n",
    "    evs (list of lists): A list of frames associated with each condition\n",
    "\n",
    "    \"\"\"\n",
    "    frames_list = []\n",
    "    task_key = f'tfMRI_{experiment}_{RUNS[run]}'\n",
    "    for cond in EXPERIMENTS[experiment]['cond']:    \n",
    "        ev_file  = f\"{HCP_DIR}/subjects/{subject}/{experiment}/{task_key}/EVs/{cond}.txt\"\n",
    "        ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
    "        ev       = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
    "        # Determine when trial starts, rounded down\n",
    "        start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
    "        # Use trial duration to determine how many frames to include for trial\n",
    "        duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
    "        # Take the range of frames that correspond to this specific trial\n",
    "        frames = [s + np.arange(0, d) for s, d in zip(start, duration)]\n",
    "        frames_list.append(frames)\n",
    "\n",
    "    return frames_list\n",
    "\n",
    "def average_frames(data, evs, experiment, cond):\n",
    "    # get the index for the requested condition\n",
    "    idx = EXPERIMENTS[experiment]['cond'].index(cond)\n",
    "    return np.mean(np.concatenate([np.mean(data[:,evs[idx][i]],axis=1,keepdims=True) for i in range(len(evs[idx]))],axis=-1),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "id": "R5gwQwxPdLst"
   },
   "source": [
    "## Step 3: Define variables for interfacing with the data\n",
    "\n",
    "For a detailed description of the tasks have a look pages 45-54 of the [HCP reference manual](https://www.humanconnectome.org/storage/app/media/documentation/s1200/HCP_S1200_Release_Reference_Manual.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtTptKa7dZFW"
   },
   "outputs": [],
   "source": [
    "# figure settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
    "\n",
    "# The data shared for NMA projects is a subset of the full HCP dataset\n",
    "N_SUBJECTS = 100\n",
    "\n",
    "# The data have already been aggregated into ROIs from the Glasser parcellation\n",
    "N_PARCELS = 360\n",
    "\n",
    "# The acquisition parameters for all tasks were identical\n",
    "TR = 0.72  # Time resolution, in seconds\n",
    "\n",
    "# The parcels are matched across hemispheres with the same order\n",
    "HEMIS = [\"Right\", \"Left\"]\n",
    "\n",
    "# Each experiment was repeated twice in each subject\n",
    "RUNS   = ['LR','RL']\n",
    "N_RUNS = 2\n",
    "\n",
    "# There are 7 tasks. Each has a number of 'conditions', the task or stimuli that were used.\n",
    "# TIP: look inside the data folders for more fine-grained information about conditions\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    'MOTOR'      : {'cond':['lf','rf','lh','rh','t','cue']},\n",
    "    'WM'         : {'cond':['0bk_body','0bk_faces','0bk_places','0bk_tools','2bk_body','2bk_faces','2bk_places','2bk_tools']},\n",
    "    'EMOTION'    : {'cond':['fear','neut']},\n",
    "    'GAMBLING'   : {'cond':['loss','win']},\n",
    "    'LANGUAGE'   : {'cond':['math','story']},\n",
    "    'RELATIONAL' : {'cond':['match','relation']},\n",
    "    'SOCIAL'     : {'cond':['ment','rnd']}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJ0ACENEwXmk"
   },
   "source": [
    "## Step 4: Get the data from Open Science Foundation\n",
    "\n",
    "The task data are shared in different files, but they will unpack into the same directory structure. In addition to the data, we will also download a `regions.npy` file, which contains a set of regions that have been defined and assigned to specific networks based on multi-modal parcellations that have been made publicly available.  \n",
    "\n",
    "Detailed information about how the regions were defined is available in [Glasser et al. 2016](https://www.nature.com/articles/nature18933), and information about the naming and functional properties of each region is provided [in the Supplement to that article](https://static-content.springer.com/esm/art%3A10.1038%2Fnature18933/MediaObjects/41586_2016_BFnature18933_MOESM330_ESM.pdf).\n",
    "\n",
    "Information about the network parcellation is provided in [Ji et al, 2019](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6289683/).\n",
    "\n",
    "### Understanding the directory structure\n",
    "\n",
    "The data folder has the following organisation:\n",
    "\n",
    "- hcp\n",
    "  - regions.npy (information on the brain parcellation)\n",
    "  - subjects_list.txt (list of subject IDs)\n",
    "  - subjects (main data folder)\n",
    "    - [subjectID] (subject-specific subfolder)\n",
    "      - EXPERIMENT (one folder per experiment)\n",
    "        - RUN (one folder per run)\n",
    "          - data.npy (the parcellated time series data)\n",
    "          - EVs (EVs folder)\n",
    "            - [ev1.txt] (one file per condition)\n",
    "            - [ev2.txt]\n",
    "            - Stats.txt (behavioural data [where available] - averaged per run)\n",
    "            - Sync.txt (ignore this file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the download cells will store the data in nested directories starting here:\n",
    "HCP_DIR = \"./hcp\"\n",
    "if not os.path.isdir(HCP_DIR):\n",
    "  os.mkdir(HCP_DIR)\n",
    "\n",
    "fname = \"hcp_task.tgz\"\n",
    "url = \"https://osf.io/2y3fw/download/\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.ConnectionError:\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            print(f\"Downloading {fname}...\")\n",
    "        with open(fname, \"wb\") as fid:\n",
    "            fid.write(r.content)\n",
    "        print(f\"Download {fname} completed!\")\n",
    "\n",
    "if not os.path.isfile(os.path.join(HCP_DIR,'subjects_list.txt')):\n",
    "    !tar -xzf $fname -C $HCP_DIR --strip-components=1\n",
    "subjects = np.loadtxt(os.path.join(HCP_DIR,'subjects_list.txt'),dtype='str')\n",
    "\n",
    "# download list of regions and define a dictionary of information about regions\n",
    "regions = np.load(f\"{HCP_DIR}/regions.npy\").T\n",
    "region_info = dict(\n",
    "    name=regions[0].tolist(),\n",
    "    network=regions[1],\n",
    "    hemi=['Right']*int(N_PARCELS/2) + ['Left']*int(N_PARCELS/2),\n",
    ")\n",
    "\n",
    "# load atlas containing the regions corresponding to the fMRI timeseries in the dataset \n",
    "url = \"https://osf.io/j5kuc/download\"\n",
    "fname = \"{}/hcp_atlas.npz\".format(HCP_DIR)\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.ConnectionError:\n",
    "        print(\"!!! Failed to download data !!!\")\n",
    "    else:\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            print(\"!!! Failed to download data !!!\")\n",
    "        else:\n",
    "            print(f\"Downloading {fname}...\")\n",
    "        with open(fname, \"wb\") as fid:\n",
    "            fid.write(r.content)\n",
    "        print(f\"Download {fname} completed!\")\n",
    "with np.load(\"{}/hcp_atlas.npz\".format(HCP_DIR)) as dobj:\n",
    "    atlas = dict(**dobj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2xGUdrAmuym"
   },
   "source": [
    "## Step 5: Plot time series of example run\n",
    "\n",
    "Let's load the timeseries data for the MOTOR experiment from a single subject and a single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-7vk9y1GfwcW",
    "outputId": "56ffd0cd-4e4f-4db6-ae15-dec8c09b1b11"
   },
   "outputs": [],
   "source": [
    "my_exp  = 'MOTOR' \n",
    "my_subj = subjects[1]\n",
    "my_run  = 1\n",
    "\n",
    "data = load_single_timeseries(subject=my_subj,experiment=my_exp,run=my_run,remove_mean=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U93-Pqllm7VU"
   },
   "source": [
    "As you can see the time series data contains 284 time points in 360 regions of interest (ROIs).\n",
    "\n",
    "Let's plot the time series within a single, randomly chosen, brain region. To do this, we also need to create a variable that describes the timing of each acquisition (TR) during the run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_points = [x*TR for x in list(range(data.shape[1]))]\n",
    "roi_to_plot = data[0]\n",
    "if len(time_points) == len(roi_to_plot):\n",
    "    print( \"roi data and time points have same length: {}\".format(len(time_points)) )\n",
    "\n",
    "time_fig = plt.figure(figsize=[12,6]) # save figure handle, given interpretable name\n",
    "plt.plot(time_points, roi_to_plot, clip_on=False)\n",
    "plt.xlim([0,200])\n",
    "plt.ylim([-150,150])\n",
    "plt.xlabel(\"time(secs)\")\n",
    "plt.ylabel('activity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVBiRQfVnqfY"
   },
   "source": [
    "## Step 6: Investigate the tasks parameters\n",
    "\n",
    "Now in order to understand how to model these data, we need to relate the time series to the experimental manipulation. This is described by the EV files. Let us load the EVs for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5bQo6IemfdI"
   },
   "outputs": [],
   "source": [
    "evs = load_evs(subject=my_subj, experiment=my_exp,run=my_run)\n",
    "print(\"number of conditions in evs is {}\".format(len(evs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKrhg4_Ervwz"
   },
   "source": [
    "For the motor task, this evs variable contains a list of 6 arrays corresponding to the 6 conditions. We can query the EXPERIMENTS dataframe we created above, to figure out what the conditions were. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_list = EXPERIMENTS[my_exp][\"cond\"]\n",
    "print(cond_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! the number of conditions in evs corresponds to the number of conditions in cond_list. Each element of **evs** is itself a list, with each element in the list being a list of the TRs where that particular condition occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"the first presentation of condition '{}' occurred at TRs:\\n {}\".format(cond_list[0], evs[0][0]) )\n",
    "print( \"the second presentation of condition '{}' occurred at TRs:\\n {}\".format(cond_list[0], evs[0][1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make that more manageable by converting to an array and the using the flatten function to make it a one-dimensional array. We use list comprehension to make a new list, **cond_times**, that has every TR during which a condition occured, for each condition. We can also make a list that counts the number of TRs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_times = [ np.array(e).flatten() for e in evs ]\n",
    "tr_count = [len(c) for c in cond_times]\n",
    "print(\"here are the number of TRs that each condition occupied during this run:\\n{}\".format(tr_count) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the final condition occured more often.\n",
    "\n",
    "Now we can add our event timing to the figure from before. Note that we use the named input variable **\"label\"** with the call to plt.plot. That assign a label to each dataseries in the plot (in this case, the events that occurred during the run) and let's use put a nice legend on the figure by simply calling plt.legend without inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(time_fig) # grab figure handle defined earlier\n",
    "for i, e in enumerate(cond_times):\n",
    "    plt.plot(e*TR, np.ones_like(e)*(150-i*10), 's', clip_on=False, label=cond_list[i])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our conditions are non-overlapping, except for cue, which makes sense giving that cue was likely what told participants to switch to a different motor behavior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Plot averaged data from different regions-of-interest (ROIs)\n",
    "\n",
    "This is cool, but difficult to interpret. Let's try to average the activity across timepoints for a specific condition. \n",
    "\n",
    "We will compare the average activity during the left foot ('lf') and right foot ('rf') conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PhLIpwe5miNX"
   },
   "outputs": [],
   "source": [
    "lf_activity = average_frames(data, evs, my_exp, 'lf')\n",
    "rf_activity = average_frames(data, evs, my_exp, 'rf')\n",
    "contrast    = lf_activity-rf_activity   # difference between left and right hand movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJ0StH_kACUw"
   },
   "outputs": [],
   "source": [
    "# Plot activity level in each ROI for both conditions\n",
    "roi_fig = plt.figure(figsize=[12,6])\n",
    "plt.plot(lf_activity,label='left foot', clip_on=False)\n",
    "plt.plot(rf_activity,label='right foot', clip_on=False)\n",
    "plt.xlabel('ROI')\n",
    "plt.ylabel('activity')\n",
    "plt.ylim([-150,150])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ERPIp7tsX-C"
   },
   "source": [
    "## Step 8: Plot averaged data from different networks of ROIs\n",
    "\n",
    "This is still quite messy. We would want to make use of the ROI names to find out which brain areas show highest activity in these conditions. But since there are so many areas, we will group them by network. We will plot the activity averaged over each network.\n",
    "\n",
    "A powerful tool for organising and plotting this data is the combination of pandas and seaborn. Below is an example where we use pandas to create a table for the activity data and we use seaborn to visualise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Og0F_4dkrsHO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.DataFrame({'lf_activity' : lf_activity,\n",
    "                   'rf_activity' : rf_activity,\n",
    "                   'network'     : region_info['network'],\n",
    "                   'hemi'        : region_info['hemi']})\n",
    "\n",
    "network_fig,(ax1,ax2) = plt.subplots(1,2, figsize=[10,10])\n",
    "sns.barplot(y='network', x='lf_activity', data=df, hue='hemi',ax=ax1)\n",
    "ax1.title.set_text(\"Left Foot\")\n",
    "sns.barplot(y='network', x='rf_activity', data=df, hue='hemi',ax=ax2)\n",
    "ax2.title.set_text(\"Right Foot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWzklttG9zp5"
   },
   "source": [
    "You should be able to notice that for the somatosensory network:\n",
    "- brain activity in the right hemisphere is higher for left compared to right foot movement \n",
    "- brain activity in the left hemisphere is higher for right compared to left foot movement \n",
    "\n",
    "But this may be subtle at the single subject/session level (these are quick 3-4min scans). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWzklttG9zp5"
   },
   "source": [
    "## Step 9: Compute group-level contrasts\n",
    "\n",
    "Let us boost these stats by averaging across all subjects and runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-6MfK_KsVgI"
   },
   "outputs": [],
   "source": [
    "group_contrast = 0\n",
    "for s in subjects:\n",
    "    for r in [0,1]:\n",
    "        data = load_single_timeseries(subject=s, experiment=my_exp, run=r, remove_mean=True)\n",
    "        evs = load_evs(subject=s, experiment=my_exp, run=r)\n",
    "\n",
    "        lf_activity = average_frames(data, evs, my_exp, 'lf')\n",
    "        rf_activity = average_frames(data, evs, my_exp, 'rf')\n",
    "\n",
    "        contrast    = lf_activity-rf_activity\n",
    "        group_contrast        += contrast # the same as group_contrast = group_contrast + contrast\n",
    "\n",
    "group_contrast = group_contrast / (len(subjects)*2)  # remember: 2 sessions per subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that we are computing a contrast: the difference between the two conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFD2BpxWsmwC"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'contrast':group_contrast,'network':region_info['network'],'hemi':region_info['hemi']})\n",
    "# we will plot the left foot minus right foot contrast so we only need one plot\n",
    "fig = plt.figure(figsize=[5,10])\n",
    "sns.barplot(y='network', x='contrast', data=df, hue='hemi')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the contrast in the somatomotor network - does it makes sense that values are strongly positive for the right hemisphere, and strongly negative for the left hemisphere?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2k8wuJmIKiCS"
   },
   "source": [
    "## Step 10: Visualising the results on a brain\n",
    "\n",
    "Finally, we will visualise these resuts on the cortical surface of a template brain. \n",
    "\n",
    "`fsaverage` is a template brain based on a combination of 40 MRI scans of real brains. We already downloaded it above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1o6dc0hqwX4n"
   },
   "outputs": [],
   "source": [
    "# load the fsaverage surface\n",
    "fsaverage = datasets.fetch_surf_fsaverage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first plot all of the regions-of-interest on a flattened hemisphere. \n",
    "\n",
    "It is important to understand that the variables atlas[\"labels_L\"] and atlas[\"labels_R\"] contains the ROI indices for each vertex on the surface (10K+ vertices per hemisphere). Simply plotting them onto the surface will show us the location of each ROI. \n",
    "\n",
    "Note that we add one to our values to map to make our ROIs take on values between 1 and 360, and that we set our range (vmin and vmax) between 181 and 360, because those are the values assigned to **left** hemisphere ROIs. If we were plotting right hemisphere ROIs, we would set the range between 1 and 180."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all regions of interest on the surface\n",
    "roi_cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
    "values_to_map = atlas[\"labels_L\"]+1\n",
    "plotting.view_surf(fsaverage['infl_left'], values_to_map, vmin=181, vmax=360, symmetric_cmap=False, cmap=roi_cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of these regions are actually part of the somatomotor network we plotted in the bar graphs above? Let's define an variable somasens_lcl, identifying those regions, and simply assign one to every index that is part of the network, and zero to anywhere else. somasens_lcl is an array of  \n",
    "\n",
    "We can now use atlas[\"labels_L\"] as a numerical index into somasens_lcl, which means that the nth value of somasens_lcl is assigned to every vertex on the left hemisphere that has value n (indicating the region). The result is that every ROI that is a member of the somatomotor network get assigned value 1, and every other vertex gets assigned value zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define index identifying somatosensory regions\n",
    "somasens_idx = [idx for idx, name in enumerate(region_info[\"network\"]) if \"Somatomotor\" in name]\n",
    "somasens_lcl = np.zeros_like(group_contrast)\n",
    "somasens_lcl[somasens_idx] = 1\n",
    "# plot somatosensory regions of interest on the surface\n",
    "values_to_map = somasens_lcl[atlas[\"labels_L\"]]\n",
    "plotting.view_surf(fsaverage['infl_left'], values_to_map, vmin=0, vmax=1, symmetric_cmap=False, cmap=roi_cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same somewhat tricky indexing principle to plot the values of the group contrast defined above on the cortical surface. Here the result is that the 360 distinct values for the group contrast (one per ROI across the whole brain) get asssigned to the ROI that has that index value. Note that you can plot right hemisphere data by simply replacing \"labels_L\" with \"labels_R\" and \"infl_left\" with \"infl_right\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot group contrast on surface\n",
    "activity_cmap = sns.color_palette(\"vlag\", as_cmap=True) # define a color map\n",
    "surf_contrast = group_contrast[atlas[\"labels_L\"]]       # indexing the contrast values with the ROI indices\n",
    "plotting.view_surf(fsaverage['infl_left'], surf_contrast, vmax=20, cmap=activity_cmap) #vmax = 20 seems good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 7\n",
    "\n",
    "The experiment we will be working with for this assignment is the working memory (**WM**) experiment. In this experiment, participants were shown blocks of trials that consisted of pictures of faces, places, tools and body parts. In each block, participants were asked to perform one of two tasks, **0-back** and **2-back**. In 0-back tasks, the participant is asked to press a button whenever a stimulus presented at the beginning of each block, appears again. In 2-back tasks, the participant is asked to press a button whenever the category is the same as the image presented two-images ago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Investigate task parameters (3 pts)\n",
    "Please copy code from **Steps 5 and 6** and modify it to do the following:\n",
    "\n",
    "a) Use load_single_timeseries to grab the data from run 2 in subject 8 in the **WM** experiment. Remember to use correct indexing, Python uses zero-based indexing. Share your code. \n",
    "\n",
    "b) Print conditions for the **WM** experiment and print the number of TRs that each condition was shown during that run. Share your code and the output of the code. \n",
    "\n",
    "c) Plot the timeseries of the run with the task blocks overlaid as in the end Section 2. It is a good idea to set the x-axis maximum to 300, rather than 150, like so: `plt.xlim([0,300])`. Share your code and a screenshot of the labeled plot. \n",
    "\n",
    "Please do not share more than one plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Run a group-level contrast (4 pts)\n",
    "Modify the code in the beginning of **Step 9**, with the two for loops, to compute a contrast between the conditions \"0bk_faces\" and \"0bk_places\" - subtracting the place response from the face response. Give your contrast variable a reasonable name, like \"zero_back_contrast\". \n",
    "\n",
    "Then duplicate and edit your code to make a new contrast, this time between the conditions \"2bk_faces\" and \"2bk_places\". Give the second contrast another name, like \"two_back_contrast\". \n",
    "\n",
    "Tip: You can keep the variable names the same inside both for loops, and then simply assign the contents of the variable \"group_contrast\" to another variable named something else at the end of each block of code. Both contrasts should now contain the difference in the response between faces and places for each of the 360 ROIs. \n",
    "\n",
    "a) Share the code you used to make the two contrasts.\n",
    "\n",
    "b) The following code\n",
    "\n",
    "        ffa_idx = [idx for idx, name in enumerate(region_info[\"name\"]) if \"_FFC\" in name]\n",
    "\n",
    "        ppa_idx = [idx for idx, name in enumerate(region_info[\"name\"]) if \"_PHA1\" in name]\n",
    "    \n",
    "creates numerical indices identifying regions of interest that approximately corresponds to the **Fusiform Face Area (FFA)** and the **Parahippocampal Place Area (PPA)**. Each variable has two indices, the lower is the right hemisphere. Use these indices to get your 0-back and 2-back contrast values for both FFA and PPA in the **left** hemisphere. Plot the values of both contrasts as two line plots on the same plot, using different colors for each contrast. The x-axis should index the values for FFA and PPA. No need for fancy bar graphs or anything, calls to\n",
    "        \n",
    "        plt.plot(...)\n",
    "\n",
    "for each contrast, will do fine. Share the code you used for the plot. \n",
    "\n",
    "c) Use \"label=' with plt.plot to label your data 'zero-back' and 'two-back' as in **Step 6**, and then add a legend to the plot. Share the code used and share the finished labeled plot. \n",
    "\n",
    "d) You may see some difference between 0- and 2-back, but the overall pattern of results across FFA and PPA should be the same for both contrasts. Please describe the pattern and explain what it says about the measured response in FFA and PPA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Plot group contrast on surface (3 pts)\n",
    "\n",
    "Modify the code at the end of **Step 10**, labeled \"plot group contrast on surface\", to plot your two-back contrast on the surface of the left hemisphere. Set `vmax = 50`. \n",
    "\n",
    "a) Take a screenshot of the left hemisphere where the ventral surface is visible. Share your screenshot and the code used to make it. \n",
    "\n",
    "b) Consider the locations of areas FFA and PPA as shown in the lecture slides for Week 08, and find those locations on the left hemisphere. Do the results of this \"whole-brain analysis\" approximately match your estimates of responses in FFA and PPA, based on the ROI analysis? Explain why/why not? \n",
    "\n",
    "c) If you look at the surface map you generated in (a), you may notice that your contrast is negative in area V1, primary visual cortex, on the posterior medial surface of the left hemisphere. Please speculate why that would be, based on your knowledge of the stimuli used in the experiment and the response preferences of early visual cortex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on your answer here\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "load_hcp_task_with_behaviour.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
